#coding:utf-8
#
# id:           tests.functional.replication.shutdown_during_applying_segments_leads_to_crash
# title:        Crash or hang while shutting down the replica database if segments are being applied
# decription:   
#                   See: https://github.com/FirebirdSQL/firebird/issues/6975
#               
#                   Bug initially was found during heavy test of replication performed by OLTP-EMUL, for FB 4.x
#                   (see letters to dimitr 13.09.2021; reply from dimitr, 18.09.2021 08:42 - all in mailbox: pz at ibase.ru).
#               
#                   It *can* be reproduced without heavy/concurrent workload, but we have to operate with data that are written
#                   into database 'slowly'. Such data can be wide INDEXED column which has GUID-based values.
#               
#                   Test creates a table with 'wide' indexed field and adds data to it.
#                   Then we save current timestamp (with accuracy up to SECONDS, i.e. cut off milli- or microseconds) to variable.
#                   After this we start check replicationb.log for appearance of phrase 'Added <N> segment(s) to the processing queue'.
#                   After founding each such phrase we skip two lines above and parse timestamp when this occurred.
#                   If timestamp in log less than saved timestamp of our DML action then we go on to the next such phrase.
#                   Otherwise we can assume that replication BEGINS to apply just generated segment.
#                   See function wait_for_add_queue_in_replica() which does this parsing of replication.log.
#               
#                   Because we operate with table which have very 'wide' index and, moreover, data in this index are GUID-generated
#                   text strings, we can safely assume that applying of segment will take at least 5...10 seconds (actually this 
#                   can take done for 30...35 seconds).
#               
#                   During this time we change replica mode to full shutdown and (immediately after that) return to online.
#                   NO message like 'error reading / writing from/to connection' must appear at this step.
#               
#                   After this, we have to wait for replica finish applying segment and when this occur we drop the table.
#               
#                   Finally, we extract metadata for master and replica and compare them (see 'f_meta_diff').
#                   The only difference in metadata must be 'CREATE DATABASE' statement with different DB names - we suppress it,
#                   thus metadata difference must not be issued.
#               
#                   ################
#                   ### N O T E  ###
#                   ################
#                   Test assumes that master and replica DB have been created beforehand.
#                   Also, it assumes that %FB_HOME%
#               eplication.conf has been prepared with apropriate parameters for replication.
#                   Particularly, name of directories and databases must have info about checked FB major version and ServerMode.
#                       * verbose = true // in order to find out line with message that required segment was replicated
#                       * section for master database with specified parameters:
#                           journal_directory = "!fbt_repo!	mp
#               b-replication.!fb_major!.!server_mode!.journal"
#                           journal_archive_directory = "!fbt_repo!	mp
#               b-replication.!fb_major!.!server_mode!.archive"
#                           journal_archive_command = "copy $(pathname) $(archivepathname)"
#                           journal_archive_timeout = 10
#                       * section for replica database with specified parameter:
#                            journal_source_directory =  "!fbt_repo!	mp
#               b-replication.!fb_major!.!server_mode!.archive"
#               
#                   Master and replica databases must be created in "!fbt_repo!	mp" directory and have names like these:
#                       'fbt-main.fb40.SS.fdb'; 'fbt-repl.fb40.SS.fdb'; - for FB 4.x ('SS' = Super; 'CS' = Classic)
#                       'fbt-main.fb50.SS.fdb'; 'fbt-repl.fb50.SS.fdb'; - for FB 5.x ('SS' = Super; 'CS' = Classic)
#                       NB: fixed numeric value ('40' or '50') must be used for any minor FB version (4.0; 4.0.1; 4.1; 5.0; 5.1 etc)
#               
#                   These two databases must NOT be dropped in any of tests related to replication!
#                   They are created and dropped in the batch scenario which prepares FB instance to be checked for each ServerMode
#                   and make cleanup after it, i.e. when all tests will be completed.
#               
#                   NB. Currently this task was implemented only in Windows batch, thus test has attribute platform = 'Windows'.
#               
#                   Temporary comment. For debug purpoces:
#                       1) find out SUFFIX of the name of FB service which is to be tested (e.g. 'DefaultInstance', '40SS' etc);
#                       2) copy file %fbt-repo%	ests
#               unctional	abloidatches\\setup-fb-for-replication.bat.txt
#                          to some place and rename it "*.bat";
#                       3) open this .bat in editor and asjust value of 'fbt_repo' variable;
#                       4) run: setup-fb-for-replication.bat [SUFFIX_OF_FB_SERVICE]
#                          where SUFFIX_OF_FB_SERVICE is ending part of FB service which you want to check:
#                          DefaultInstance ; 40ss ; 40cs ; 50ss ; 50cs etc
#                       5) batch 'setup-fb-for-replication.bat' will:
#                          * stop selected FB instance
#                          * create test databases (in !fbt_repo!	mp\\);
#                          * prepare journal/archive sub-folders for replication (also in !fbt_repo!	mp\\);
#                          * replace %fb_home%
#               eplication.conf with apropriate
#                          * start selected FB instance
#                       6) run this test (FB instance will be already launched by setup-fb-for-replication.bat):
#                           %fpt_repo%
#               bt-run2.bat dblevel-triggers-must-not-fire-on-replica.fbt 50ss, etc
#               
#               	Confirmed bug on 5.0.0.215: server crashed when segment was applied to replica and at the same time we issued
#               	'gfix -shut full -force 0 ...'. Regardless of that command, replica DB remained in NORMAL mode, not in shutdown.
#               	If this command was issued after this again - FB process hanged (gfix could not return control to OS).
#               	This is the same bug as described in the ticked (discussed with dimitr, letters 22.09.2021).
#                   
#                   Checked on: 4.0.1.2613 (SS/CS); 5.0.0.219 (SS/CS)
#                
# tracker_id:   
# min_versions: ['4.0.1']
# versions:     4.0.1
# qmid:         None

import pytest
from firebird.qa import db_factory, python_act, Action

# version: 4.0.1
# resources: None

substitutions_1 = [('Start removing objects in:.*', 'Start removing objects'), ('Finish. Total objects removed:  [1-9]\\d*', 'Finish. Total objects removed'), ('.* CREATE DATABASE .*', ''), ('FMT_DESCR .*', 'FMT_DESCR'), ('[ \t]+', ' ')]

init_script_1 = """"""

db_1 = db_factory(sql_dialect=3, init=init_script_1)

# test_script_1
#---
# 
#  import os
#  import subprocess
#  import re
#  import difflib
#  import shutil
#  import time
#  from datetime import datetime
#  from datetime import timedelta
#  
#  
#  os.environ["ISC_USER"] = user_name
#  os.environ["ISC_PASSWORD"] = user_password
#  
#  # NB: with default values of 'apply_idle_timeout' and 'apply_error_timeout' (10 and 60 s)
#  # total time of this test is about 130...132s.
#  #####################################
#  MAX_TIME_FOR_WAIT_SEGMENT_IN_LOG = 135
#  MAX_TIME_FOR_WAIT_ADDED_TO_QUEUE = 135
#  #####################################
#  
#  svc = fdb.services.connect(host='localhost', user=user_name, password=user_password)
#  FB_HOME = svc.get_home_directory()
#  svc.close()
#  
#  engine = db_conn.engine_version         # 4.0; 4.1; 5.0 etc -- type float
#  fb_major = 'fb' + str(engine)[:1] + '0' # 'fb40'; 'fb50'
#  
#  cur = db_conn.cursor()
#  cur.execute("select rdb$config_value from rdb$config where upper(rdb$config_name) = upper('ServerMode')")
#  server_mode = 'XX'
#  for r in cur:
#      if r[0] == 'Super':
#          server_mode = 'SS'
#      elif r[0] == 'SuperClassic':
#          server_mode = 'SC'
#      elif r[0] == 'Classic':
#          server_mode = 'CS'
#  cur.close()
#  
#  # 'fbt-main.fb50.ss.fdb' etc:
#  db_main = os.path.join( context['temp_directory'], 'fbt-main.' + fb_major + '.' + server_mode + '.fdb' )
#  db_repl = db_main.replace( 'fbt-main.', 'fbt-repl.')
#  
#  # Folders for journalling and archieving segments.
#  repl_journal_dir = os.path.join( context['temp_directory'], 'fb-replication.' + fb_major + '.' + server_mode + '.journal' )
#  repl_archive_dir = os.path.join( context['temp_directory'], 'fb-replication.' + fb_major + '.' + server_mode +  '.archive' )
#  
#  runProgram('gfix', ['-w', 'async', 'localhost:' + db_main])
#  
#  db_conn.close()
#  
#  #--------------------------------------------
#  
#  def flush_and_close( file_handle ):
#      # https://docs.python.org/2/library/os.html#os.fsync
#      # If you're starting with a Python file object f,
#      # first do f.flush(), and
#      # then do os.fsync(f.fileno()), to ensure that all internal buffers associated with f are written to disk.
#      global os
#  
#      file_handle.flush()
#      if file_handle.mode not in ('r', 'rb') and file_handle.name != os.devnull:
#          # otherwise: "OSError: [Errno 9] Bad file descriptor"!
#          os.fsync(file_handle.fileno())
#      file_handle.close()
#  
#  #--------------------------------------------
#  
#  def cleanup( f_names_list ):
#      global os
#      for i in range(len( f_names_list )):
#         if type(f_names_list[i]) == file:
#            del_name = f_names_list[i].name
#         elif type(f_names_list[i]) == str:
#            del_name = f_names_list[i]
#         else:
#            print('Unrecognized type of element:', f_names_list[i], ' - can not be treated as file.')
#            del_name = None
#  
#         if del_name and os.path.isfile( del_name ):
#             os.remove( del_name )
#  
#  #--------------------------------------------
#  
#  def wait_for_data_in_replica( fb_home, max_allowed_time_for_wait, db_main, prefix_msg = '' ):
#      global re
#      global difflib
#      global time
#  
#      # -:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-
#      def check_pattern_in_log( log_lines, pattern, prefix_msg = '' ):
#          found_required_message = False
#          for d in log_lines:
#              if pattern.search(d):
#                  print( (prefix_msg + ' ' if prefix_msg else '') + 'FOUND message about replicated segment.' )
#                  found_required_message = True
#                  break
#          return found_required_message
#      # -:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-
#  
#      replold_lines = []
#      with open( os.path.join(fb_home,'replication.log'), 'r') as f:
#          replold_lines = f.readlines()
#  
#      con = fdb.connect( dsn = 'localhost:' + db_main, no_db_triggers = 1)
#      cur = con.cursor()
#      cur.execute("select rdb$get_context('SYSTEM','REPLICATION_SEQUENCE') from rdb$database")
#      for r in cur:
#          last_generated_repl_segment = r[0]
#      cur.close()
#      con.close()
#  
#      # VERBOSE: Segment 1 (2582 bytes) is replicated in 1 second(s), deleting the file
#      segment_replicated_pattern=re.compile( 'verbose:\\s+segment\\s+%(last_generated_repl_segment)s\\s+\\(\\d+\\s+bytes\\)\\s+is\\s+replicated.*deleting' % locals(), re.IGNORECASE)
#  
#      # 08.09.2021: replication content can remain unchanged if there was no user-defined object in DB that must be dropped!
#      # Because of this, it is crucial to check OLD content of replication log before loop.
#      # Also, segment_replicated_pattern must NOT start from '\\+' because it can occur only for diff_data (within loop):
#      #
#      found_required_message = check_pattern_in_log( replold_lines, segment_replicated_pattern, prefix_msg )
#  
#      if not found_required_message:
#  
#          for i in range(0,max_allowed_time_for_wait):
#              time.sleep(1)
#           
#              # Get content of fb_home replication.log _after_ isql finish:
#              f_repllog_new = open( os.path.join(fb_home,'replication.log'), 'r')
#              diff_data = difflib.unified_diff(
#                  replold_lines, 
#                  f_repllog_new.readlines()
#                )
#              f_repllog_new.close()
#  
#              found_required_message = check_pattern_in_log( diff_data, segment_replicated_pattern, prefix_msg )
#              if found_required_message:
#                  break
#  
#      if not found_required_message:
#          print('UNEXPECTED RESULT: no message about replicated segment No. %d for %d seconds.' % (int(last_generated_repl_segment), max_allowed_time_for_wait) )
#  
#  
#  #--------------------------------------------
#  
#  def wait_for_add_queue_in_replica( fb_home, max_allowed_time_for_wait, min_timestamp, prefix_msg = '' ):
#  
#      global re
#      global difflib
#      global time
#      global datetime
#  
#      # <hostname> (replica) Tue Sep 21 20:24:57 2021
#      # Database: ...
#      # Added 3 segment(s) to the processing queue
#  
#      # -:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-
#      def check_pattern_in_log( log_lines, pattern, min_timestamp, prefix_msg = '' ):
#          found_required_message = False
#          for i,r in enumerate(log_lines):
#              if pattern.search(r):
#                  if i>=2 and log_lines[i-2]:
#                      # a = r.replace('(',' ').split()
#                      a = log_lines[i-2].split()
#                      if len(a)>=4:
#                          # s='replica_host_name (slave) Sun May 30 17:46:43 2021'
#                          # s.split()[-5:] ==> ['Sun', 'May', '30', '17:46:43', '2021']
#                          # ' '.join( ...) ==> 'Sun May 30 17:46:43 2021'
#                          dts = ' '.join( log_lines[i-2].split()[-5:] )
#                          segment_timestamp = datetime.strptime( dts, '%a %b %d %H:%M:%S %Y')
#                          if segment_timestamp >= min_timestamp:
#                              print( (prefix_msg + ' ' if prefix_msg else '') + 'FOUND message about segments added to queue after given timestamp.') #, 'segment_timestamp=%s' % segment_timestamp, '; min_timestamp=%s' % min_timestamp )
#                              found_required_message = True
#                              break
#          return found_required_message
#  
#      # -:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-:-
#  
#      replold_lines = []
#      with open( os.path.join(fb_home,'replication.log'), 'r') as f:
#          replold_lines = f.readlines()
#  
#  
#      segments_to_queue_pattern=re.compile( 'verbose:\\s+added\\s+\\d+\\s+segment.*to.*queue', re.IGNORECASE)
#  
#      # 08.09.2021: replication content can remain unchanged if there was no user-defined object in DB that must be dropped!
#      # Because of this, it is crucial to check OLD content of replication log before loop.
#      # Also, segments_to_queue_pattern must NOT start from '\\+' because it can occur only for diff_data (within loop):
#      #
#      found_required_message = check_pattern_in_log( replold_lines, segments_to_queue_pattern, min_timestamp, prefix_msg )
#  
#      if not found_required_message:
#  
#          for i in range(0,max_allowed_time_for_wait):
#              time.sleep(1)
#           
#              # Get content of fb_home replication.log _after_ isql finish:
#              f_repllog_new = open( os.path.join(fb_home,'replication.log'), 'r')
#              diff_data = difflib.unified_diff(
#                  replold_lines, 
#                  f_repllog_new.readlines()
#                )
#              f_repllog_new.close()
#  
#              found_required_message = check_pattern_in_log( list(diff_data), segments_to_queue_pattern, min_timestamp, prefix_msg )
#              if found_required_message:
#                  break
#  
#      if not found_required_message:
#          print('UNEXPECTED RESULT: no message about segments added to queue after %s.' % min_timestamp)
#  
#  #--------------------------------------------
#  
#  sql_ddl = '''    set bail on;
#      recreate table test(s varchar(700), constraint test_s_unq unique(s));
#      commit;
#  
#      set term ^;
#      execute block as
#          declare fld_len int;
#          declare n int;
#      begin
#          select ff.rdb$field_length
#          from rdb$relation_fields rf
#          join rdb$fields ff on rf.rdb$field_source = ff.rdb$field_name
#          where upper(rf.rdb$relation_name) = upper('test') and upper(rf.rdb$field_name) = upper('s')
#          into fld_len;
#  
#  
#          n = 10000;
#          while (n > 0) do
#          begin
#              insert into test(s) values( lpad('', :fld_len, uuid_to_char(gen_uuid())) );
#              n = n - 1;
#          end
#  
#      end
#      ^
#      set term ;^
#      commit;
#  ''' % locals()
#  
#  
#  f_sql_chk = open( os.path.join(context['temp_directory'],'tmp_gh_6975_init.sql'), 'w')
#  f_sql_chk.write(sql_ddl)
#  flush_and_close( f_sql_chk )
#  
#  f_sql_log = open( ''.join( (os.path.splitext(f_sql_chk.name)[0], '.log' ) ), 'w')
#  f_sql_err = open( ''.join( (os.path.splitext(f_sql_chk.name)[0], '.err' ) ), 'w')
#  subprocess.call( [ context['isql_path'], 'localhost:' + db_main, '-i', f_sql_chk.name ], stdout = f_sql_log, stderr = f_sql_err)
#  flush_and_close( f_sql_log )
#  flush_and_close( f_sql_err )
#  
#  last_generated_repl_segment = 0
#  
#  with open(f_sql_err.name,'r') as f:
#      for line in f:
#          print('UNEXPECTED STDERR in initial SQL: ' + line)
#          MAX_TIME_FOR_WAIT_SEGMENT_IN_LOG = 0
#  
#  if MAX_TIME_FOR_WAIT_SEGMENT_IN_LOG: # ==> initial SQL script finished w/o errors
#  
#  
#      ##################################
#      ###  A.C.H.T.U.N.G             ###
#      ### do NOT use datetime.now()  ###
#      ### because of missed accuracy ###
#      ### of timestamps in repl.log  ###
#      ### (it is HH:MM:SS only)      ###
#      ##################################
#      current_date_with_hhmmss = datetime.today().replace(microsecond=0)
#  
#      
#      ##############################################################################
#      ###  W A I T   F O R    S E G M E N T S    A D D E D    T O    Q U E U E   ###
#      ##############################################################################
#      wait_for_add_queue_in_replica( FB_HOME, MAX_TIME_FOR_WAIT_ADDED_TO_QUEUE, current_date_with_hhmmss, 'POINT-A' )
#  
#      # This led to crash and appearance of message:
#      # "Fatal lock manager error: invalid lock id (0), errno: 0" in firebird.log:
#      #
#      runProgram('gfix', ['-shut', 'full', '-force', '0', 'localhost:' + db_repl])
#  
#      f_repl_hdr_log=open( os.path.join(context['temp_directory'],'db_repl_hdr.log'), 'w')
#      subprocess.call( [context['gstat_path'], db_repl, '-h'], stdout=f_repl_hdr_log, stderr=subprocess.STDOUT )
#      flush_and_close( f_repl_hdr_log )
#  
#      with open(f_repl_hdr_log.name,'r') as f:
#          for line in f:
#              if 'Attributes' in line:
#                  print('POINT-B ' + line.strip())
#  
#  
#      # This (issuing 'gfix -shu ...' second time) led FB process to hang!
#      # runProgram('gfix', ['-shut', 'full', '-force', '0', 'localhost:' + db_repl])
#  
#      runProgram('gfix', ['-online', 'localhost:' + db_repl])
#  
#  
#      ##############################################################################
#      ###  W A I T   U N T I L    R E P L I C A    B E C O M E S   A C T U A L   ###
#      ##############################################################################
#      wait_for_data_in_replica( FB_HOME, MAX_TIME_FOR_WAIT_SEGMENT_IN_LOG, db_main, 'POINT-1' )
#  
#  # return initial state of master DB:
#  # remove all DB objects (tables, views, ...):
#  # --------------------------------------------
#  sql_clean_ddl = os.path.join(context['files_location'],'drop-all-db-objects.sql')
#  
#  f_clean_log=open( os.path.join(context['temp_directory'],'drop-all-db-objects-gh_6975.log'), 'w')
#  f_clean_err=open( ''.join( ( os.path.splitext(f_clean_log.name)[0], '.err') ), 'w')
#  subprocess.call( [context['isql_path'], 'localhost:' + db_main, '-q', '-nod', '-i', sql_clean_ddl], stdout=f_clean_log, stderr=f_clean_err )
#  flush_and_close(f_clean_log)
#  flush_and_close(f_clean_err)
#  
#  with open(f_clean_err.name,'r') as f:
#      for line in f:
#          print('UNEXPECTED STDERR in cleanup SQL: ' + line)
#          MAX_TIME_FOR_WAIT_SEGMENT_IN_LOG = 0
#  
#  with open(f_clean_log.name,'r') as f:
#      for line in f:
#          # show number of dropped objects
#          print(line)
#  
#  if MAX_TIME_FOR_WAIT_SEGMENT_IN_LOG: # ==> previous SQL script finished w/o errors
#  
#      ##############################################################################
#      ###  W A I T   U N T I L    R E P L I C A    B E C O M E S   A C T U A L   ###
#      ##############################################################################
#      wait_for_data_in_replica( FB_HOME, MAX_TIME_FOR_WAIT_SEGMENT_IN_LOG, db_main, 'POINT-2' )
#  
#      f_main_meta_sql=open( os.path.join(context['temp_directory'],'db_main_meta_gh_6975.sql'), 'w')
#      subprocess.call( [context['isql_path'], 'localhost:' + db_main, '-q', '-nod', '-ch', 'utf8', '-x'], stdout=f_main_meta_sql, stderr=subprocess.STDOUT )
#      flush_and_close( f_main_meta_sql )
#  
#      f_repl_meta_sql=open( os.path.join(context['temp_directory'],'db_repl_meta_gh_6975.sql'), 'w')
#      subprocess.call( [context['isql_path'], 'localhost:' + db_repl, '-q', '-nod', '-ch', 'utf8', '-x'], stdout=f_repl_meta_sql, stderr=subprocess.STDOUT )
#      flush_and_close( f_repl_meta_sql )
#  
#      db_main_meta=open(f_main_meta_sql.name, 'r')
#      db_repl_meta=open(f_repl_meta_sql.name, 'r')
#  
#      diffmeta = ''.join(difflib.unified_diff(
#          db_main_meta.readlines(), 
#          db_repl_meta.readlines()
#        ))
#      db_main_meta.close()
#      db_repl_meta.close()
#  
#      f_meta_diff=open( os.path.join(context['temp_directory'],'db_meta_diff_gh_6975.txt'), 'w', buffering = 0)
#      f_meta_diff.write(diffmeta)
#      flush_and_close( f_meta_diff )
#  
#      # Following must issue only TWO rows:
#      #     UNEXPECTED METADATA DIFF.: -/* CREATE DATABASE 'localhost:[db_main]' ... */
#      #     UNEXPECTED METADATA DIFF.: -/* CREATE DATABASE 'localhost:[db_repl]' ... */
#      # Only thes lines will be suppressed further (see subst. section):
#      with open(f_meta_diff.name, 'r') as f:
#          for line in f:
#             if line[:1] in ('-', '+') and line[:3] not in ('---','+++'):
#                 print('UNEXPECTED METADATA DIFF.: ' + line)
#  
#  runProgram('gfix', ['-w', 'sync', 'localhost:' + db_main])
#  
#  ######################
#  ### A C H T U N G  ###
#  ######################
#  # MANDATORY, OTHERWISE REPLICATION GETS STUCK ON SECOND RUN OF THIS TEST
#  # WITH 'ERROR: Record format with length 68 is not found for table TEST':
#  runProgram('gfix', ['-sweep', 'localhost:' + db_repl])
#  runProgram('gfix', ['-sweep', 'localhost:' + db_main])
#  #######################
#  
#  # cleanup:
#  ##########
#  #cleanup( (f_sql_chk, f_sql_log, f_sql_err,f_clean_log,f_clean_err,f_main_meta_sql,f_repl_meta_sql,f_meta_diff) )
#    
#---
act_1 = python_act('db_1', substitutions=substitutions_1)

expected_stdout_1 = """
    POINT-A FOUND message about segments added to queue after given timestamp.
    POINT-B Attributes            force write, full shutdown, read-only replica
    POINT-1 FOUND message about replicated segment.
    Start removing objects
    Finish. Total objects removed
    POINT-2 FOUND message about replicated segment.
"""

@pytest.mark.version('>=4.0.1')
@pytest.mark.platform('Windows')
@pytest.mark.xfail
def test_1(act_1: Action):
    pytest.fail("Test not IMPLEMENTED")


